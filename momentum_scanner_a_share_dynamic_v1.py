#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå› å­è®¡ç®—å¼•æ“
- å¤šç»´åº¦ç‰¹å¾å·¥ç¨‹
- æ»šåŠ¨è®­ç»ƒçª—å£+æ ·æœ¬æƒé‡
- è¿‡æ‹Ÿåˆæ§åˆ¶
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import logging
import numba

# <<<--- NumbaåŠ é€ŸOLSè®¡ç®— ---<<<
@numba.jit(nopython=True)
def fast_ols_std(y, x):
    if len(y) < 2:
        return np.nan
    X = np.ones((len(x), 2))
    X[:, 1] = x
    XTX = X.T @ X
    det = np.linalg.det(XTX)
    if np.abs(det) < 1e-8:
        return np.nan
    try:
        beta = np.linalg.inv(XTX) @ X.T @ y
        residuals = y - X @ beta
        return np.std(residuals)
    except:
        return np.nan

def calc_iv(ret_1d: pd.Series, mkt_1d: pd.Series) -> float:
    """è®¡ç®—ç‰¹è´¨æ³¢åŠ¨ç‡"""
    df = pd.DataFrame({'ret': ret_1d, 'mkt': mkt_1d}).dropna()
    if len(df) < 60:
        return np.nan
    return fast_ols_std(df['ret'].values, df['mkt'].values)

# å…¨å±€å‚æ•°
MIN_TRAIN_DAYS = 500
LGB_PARAMS = {
    'objective': 'binary',
    'n_estimators': 200,
    'learning_rate': 0.03,
    'num_leaves': 31,
    'max_depth': 5,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_samples': 50,
    'random_state': 42,
    'verbose': -1,
    'n_jobs': 1
}

def calculate_factors_for_single_symbol(symbol: str, price_df: pd.DataFrame, mkt_ret_series: pd.Series):
    """
    æ ¸å¿ƒå¢å¼ºç‰ˆï¼šä¸ºå•åªè‚¡ç¥¨è®¡ç®—å› å­
    """
    g = price_df[price_df['symbol'] == symbol][['trade_date', 'close', 'total_mv', 'deduct_parent_netprofit', 'volume', 'symbol']]
    if len(g) < MIN_TRAIN_DAYS:
        logging.debug(f'{symbol} æ•°æ®ä¸è¶³{MIN_TRAIN_DAYS}å¤©')
        return None
    
    fac = calc_daily_factor_monthly_train(g, mkt_ret_series)
    if fac is not None and not fac.empty:
        fac['symbol'] = symbol
        return fac
    return None

def calc_daily_factor_monthly_train(df_sym: pd.DataFrame, mkt_ret_series: pd.Series) -> pd.DataFrame:
    """å¢å¼ºç‰ˆï¼šå¤šç»´åº¦ç‰¹å¾ + æ»šåŠ¨è®­ç»ƒ"""
    df = df_sym.copy().set_index('trade_date')
    df['return'] = df['close'].pct_change()
    df = df.merge(mkt_ret_series.rename('mkt_ret'), left_index=True, right_index=True, how='left')
    
    # <<<--- ç‰¹å¾å·¥ç¨‹å¢å¼º ---<<<
    
    # 1. åŠ¨é‡ç‰¹å¾ï¼ˆå¤šå‘¨æœŸï¼‰
    df['return_1m'] = df['return'].rolling(21).sum()
    df['return_3m'] = df['return'].rolling(63).sum()
    df['return_6m'] = df['return'].rolling(126).sum()
    df['return_12m'] = df['return'].rolling(252).sum()
    
    # 2. æ³¢åŠ¨ç‡ç‰¹å¾
    df['vol_1m'] = df['return'].rolling(21).std()
    df['vol_3m'] = df['return'].rolling(63).std()
    df['vol_12m'] = df['return'].rolling(252).std()
    df['vol_ratio'] = df['vol_1m'] / df['vol_12m'].clip(lower=0.001)
    
    # 3. ä»·æ ¼ä½ç½®ç‰¹å¾ï¼ˆå¤šå‘¨æœŸï¼‰
    for window in [63, 126, 252]:
        rolling_max = df['close'].rolling(window, min_periods=window//2).max()
        rolling_min = df['close'].rolling(window, min_periods=window//2).min()
        df[f'price_pos_{window}d'] = (df['close'] - rolling_min) / (rolling_max - rolling_min).clip(lower=1e-8)
    
    # 4. æµåŠ¨æ€§ç‰¹å¾
    df['turnover'] = (df['volume'] * df['close']) / df['total_mv']
    df['turnover_1m_avg'] = df['turnover'].rolling(21).mean()
    
    # 5. è¶‹åŠ¿å¼ºåº¦ç‰¹å¾ï¼ˆ0-3åˆ†ï¼‰
    df['ma_20'] = df['close'].rolling(20).mean()
    df['ma_60'] = df['close'].rolling(60).mean()
    df['ma_120'] = df['close'].rolling(120).mean()
    df['trend_score'] = (df['ma_20'] > df['ma_60']).astype(int) + \
                       (df['ma_60'] > df['ma_120']).astype(int) + \
                       (df['close'] > df['ma_20']).astype(int)
    
    # 6. RSIç‰¹å¾
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / loss.clip(lower=0.001)
    df['rsi_14'] = 100 - (100 / (1 + rs))
    
    # 7. MACDç‰¹å¾
    exp1 = df['close'].ewm(span=12).mean()
    exp2 = df['close'].ewm(span=26).mean()
    df['macd'] = exp1 - exp2
    df['macd_signal'] = df['macd'].ewm(span=9).mean()
    df['macd_hist'] = df['macd'] - df['macd_signal']
    
    # 8. ä¸šç»©ç‰¹å¾
    df['profit_yoy'] = df['deduct_parent_netprofit'].pct_change(periods=252).fillna(0)
    df['profit_2yoy'] = df['deduct_parent_netprofit'].pct_change(periods=504).fillna(0)
    df['profit_ttm'] = df['deduct_parent_netprofit'] / df['total_mv']
    df['log_mv'] = np.log(df['total_mv'].clip(lower=1))
    
    # 9. åŠ¨é‡è¿æ¶¨è¿è·Œç‰¹å¾
    max_dur = 12
    df['sign'] = np.where(df['return'] > 0, 1, -1)
    df['streak_start'] = (df['sign'] != df['sign'].shift(1)).cumsum()
    df['raw_streak'] = df.groupby(['streak_start']).cumcount() + 1
    df['raw_streak'] = np.minimum(df['raw_streak'], max_dur)
    df['P_d'] = np.where(df['sign'] == 1, df['raw_streak'], 0)
    df['N_d'] = np.where(df['sign'] == -1, df['raw_streak'], 0)
    
    # <<<--- æ»åæ‰€æœ‰ç‰¹å¾ï¼ˆé˜²æœªæ¥å‡½æ•°ï¼‰ ---<<<
    feature_base = [
        'return_1m', 'return_6m',
        'vol_ratio',  # æ³¢åŠ¨ç‡å˜åŒ–ï¼ˆé‡ç¼©ä»·ç¨³ï¼‰
        'price_pos_126d',  # åŠå¹´ä»·æ ¼ä½ç½®ï¼ˆä½ä½åè½¬ï¼‰
        'turnover_1m_avg', # æ¢æ‰‹ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼Œæˆ–æé«˜æ—¶çš„åè½¬ï¼‰
        'profit_ttm', # ä¼°å€¼/ç›ˆåˆ©èƒ½åŠ›ï¼ˆé˜²é›·ï¼‰ 
        'log_mv' # å¸‚å€¼
    ]
    
    for feat in feature_base:
        df[f'{feat}_lag'] = df[feat].shift(1)
    
    # æ»åå…¶ä»–ç‰¹å¾
    df['P_d_t1'] = df['P_d'].shift(1)
    df['N_d_t1'] = df['N_d'].shift(1)
    df['rm_t1'] = df['mkt_ret'].shift(1)
    
    # IVè®¡ç®—
    df['IV'] = df['return'].expanding(MIN_TRAIN_DAYS).apply(
        lambda r: calc_iv(r, df.loc[r.index, 'mkt_ret']), raw=False)
    df['IV_t1'] = df['IV'].shift(1)
    
    # æœ€ç»ˆç‰¹å¾åˆ—è¡¨
    features = [f'{feat}_lag' for feat in feature_base] + \
               ['IV_t1']
    
    # æ•°æ®æ¸…æ´—
    df_clean = df.dropna(subset=features).copy()
    df_clean['target'] = (df_clean['return'].rolling(21).sum().shift(-21) > 0).astype(int)
    
    if len(df_clean) < MIN_TRAIN_DAYS + 21:
        logging.debug(f"æ•°æ®ä¸è¶³ï¼š{len(df_clean)}å¤©")
        return pd.DataFrame()
    
    # <<<--- æ»šåŠ¨è®­ç»ƒ + æ ·æœ¬æƒé‡ ---<<<
    all_factors = []
    scaler = StandardScaler()
    training_dates = df_clean.index.to_period('M').unique().to_timestamp(how='end')
    
    # <<<--- ğŸ”¥ æ–°å¢ 1ï¼šåˆå§‹åŒ–é‡è¦æ€§è®°å½•åˆ—è¡¨ ---<<<
    feature_importance_list = []

    for i, train_end_date in enumerate(training_dates):
        # è·å–å½“å‰æœˆä»½æ‰€æœ‰äº¤æ˜“æ—¥
        valid_dates = df_clean.index[df_clean.index <= train_end_date]
        if valid_dates.empty:
            continue
        
        actual_train_end_date = valid_dates[-1]
        # 2. ã€æ ¸å¿ƒä¿®å¤ã€‘è®¾ç½®éš”ç¦»æœŸï¼ˆGapï¼‰ï¼Œé˜²æ­¢æ ‡ç­¾æ³„éœ²
        # å¿…é¡»å›é€€ 30 å¤©ï¼ˆæˆ– 21 ä¸ªäº¤æ˜“æ—¥ä»¥ä¸Šï¼‰ï¼Œå› ä¸ºä½ çš„ Target æ˜¯ shift(-21)
        safe_train_end_date = actual_train_end_date - pd.Timedelta(days=30)
        
        # 3. ã€æ—¥æœŸè®¾å®šã€‘è®¡ç®—è®­ç»ƒå¼€å§‹æ—¥æœŸ
        # é€»è¾‘ï¼šä»â€œå®‰å…¨æˆªæ­¢æ—¥â€å¾€å‰æ¨ N ä¸ªæœˆ
        # å»ºè®®ï¼šAè‚¡ä¸€è½®ç‰›ç†Šé€šå¸¸ 3-5 å¹´ï¼Œå»ºè®®è®¾ä¸º 60 ä¸ªæœˆï¼ˆ5å¹´ï¼‰èƒ½è®©æ¨¡å‹æ›´ç¨³å¥
        # å¦‚æœè¿½æ±‚è¿ç®—é€Ÿåº¦ï¼Œç»´æŒ 36 ä¸ªæœˆä¹Ÿå¯ä»¥
        train_window_months = 60  
        train_start_date = safe_train_end_date - pd.DateOffset(months=train_window_months)
        
        # 4. åˆ‡åˆ†æ•°æ®
        train_df = df_clean.loc[train_start_date:safe_train_end_date]
        
        if len(train_df) < MIN_TRAIN_DAYS:
            continue
        
        X_train = train_df[features].dropna()
        y_train = train_df.loc[X_train.index, 'target'].dropna()
        
        if y_train.nunique() < 2:
            continue
        
        # æ ·æœ¬æƒé‡ï¼šè¿‘æœŸæ ·æœ¬æƒé‡æ›´é«˜
        date_weights = np.exp(-0.01 * np.arange(len(y_train))[::-1])
        sample_weights = date_weights / date_weights.sum() * len(date_weights)
        
        # è®­ç»ƒæ¨¡å‹
        X_train_scaled = scaler.fit_transform(X_train)
        model = lgb.LGBMClassifier(**LGB_PARAMS)
        model.fit(X_train_scaled, y_train, sample_weight=sample_weights[:len(X_train)])

        # <<<--- ğŸ”¥ æ–°å¢ 2ï¼šè®°å½•å½“å‰æœˆä»½æ¨¡å‹æœ€çœ‹é‡ä»€ä¹ˆ ---<<<
        # æå–å½“å‰æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        imp_df = pd.DataFrame({
            'feature': features,
            'gain': model.booster_.feature_importance(importance_type='gain'), # gainè¡¨ç¤ºè´¡çŒ®äº†å¤šå°‘æ”¶ç›Š
            'date': actual_train_end_date
        })
        feature_importance_list.append(imp_df)
        
        # é¢„æµ‹ä¸‹æœˆ
        pred_start_date = actual_train_end_date + pd.Timedelta(days=1)
        pred_end_date = pred_start_date + pd.offsets.MonthEnd(1)
        pred_df = df_clean.loc[pred_start_date:pred_end_date]
        
        if pred_df.empty:
            continue
        
        X_pred = pred_df[features]
        pred_probas = model.predict_proba(scaler.transform(X_pred))[:, 1]
        month_factors = pd.DataFrame(pred_probas, index=X_pred.index, columns=['factor'])
        all_factors.append(month_factors)
    
        # <<<--- ğŸ”¥ æ–°å¢ 3ï¼šå¾ªç¯ç»“æŸåï¼Œæ±‡æ€»å¹¶æ‰“å°å¹³å‡é‡è¦æ€§ ---<<<
        if feature_importance_list:
            all_imp = pd.concat(feature_importance_list)
            # æŒ‰ç‰¹å¾åˆ†ç»„å–å¹³å‡å€¼ï¼Œä»å¤§åˆ°å°æ’åº
            summary = all_imp.groupby('feature')['gain'].mean().sort_values(ascending=False)
            
            print("\n" + "="*40)
            print(f"ğŸ“Š æ¨¡å‹å¿ƒä¸­çš„â€œé€‰è‚¡ç§˜ç±â€ (Symbol: {df_sym['symbol'].iloc[0]})")
            print("="*40)
            # è®¡ç®—ç™¾åˆ†æ¯”ï¼Œæ›´ç›´è§‚
            print((summary / summary.sum() * 100).apply(lambda x: f"{x:.1f}%"))
            print("="*40 + "\n")
    if not all_factors:
        return pd.DataFrame()
    
    final_factors = pd.concat(all_factors)
    return final_factors.reset_index()